{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOeaI2MVoSB2ot6BDAMma6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heyibad/genai-projects/blob/main/rag_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk67mCNYjrAb",
        "outputId": "b2604768-88c6-4b3d-aaf4-97c4fb607b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "gemini_api= userdata.get('GEMINI_API_KEY')\n",
        "pinecone_api= userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "plPCuxqik18X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
        "        metadata={\"source\": \"fish-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
        "        metadata={\"source\": \"bird-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "wQ2Qp7ELlVRE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",\n",
        "                                          google_api_key=gemini_api)"
      ],
      "metadata": {
        "id": "ePxi2jL2lxPa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_client= Pinecone(api_key=pinecone_api)"
      ],
      "metadata": {
        "id": "FCkAnSwemlqW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"rag-02\"\n",
        "\n",
        "pinecone_client.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )"
      ],
      "metadata": {
        "id": "bfeqNm3QnJfY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone_client.Index(index_name)"
      ],
      "metadata": {
        "id": "gh7b1Luhnk6P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "FF-EcZg0mCbw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vectorstore.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiGLaa3UoKjm",
        "outputId": "b56b6402-f306-4eac-b01d-3b5337927c72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8e7053c3-cf37-4f04-9a34-7a4e37592190',\n",
              " '18f57382-3d7a-4049-8953-725185beba7d',\n",
              " '60c438df-a982-4024-82cf-0e029be60216',\n",
              " 'c7832f9b-3f86-46e2-be7b-0087989b3d6e',\n",
              " 'cba607a2-d11f-4494-aea8-43e9773f9aaa']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document: This is likely a class representing a document object, which could contain text, metadata, or other information.\n",
        "\n",
        "RunnableLambda: This is a utility that allows you to wrap a function (or callable) into a \"runnable\" object, which can be used in a pipeline or chain of operations."
      ],
      "metadata": {
        "id": "rtCIoXoJrvER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)"
      ],
      "metadata": {
        "id": "4jrvu8coogHm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the batch method processes multiple queries at once. Here, it will perform a similarity search for the query \"shark\" and return the top 1 most similar document(s) from the vector store."
      ],
      "metadata": {
        "id": "_lG3hCRIpcoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.batch([\"shark\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_o2UOpvo3FJ",
        "outputId": "36cadaa9-1998-47f9-e2d6-965ec7184efc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='60c438df-a982-4024-82cf-0e029be60216', metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             api_key = gemini_api)"
      ],
      "metadata": {
        "id": "aUz9XCGFpnED"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate: A utility for creating structured prompts that can be used with chat-based language models.\n",
        "\n",
        "RunnablePassthrough: A utility that allows you to pass data through a pipeline without modifying it. It’s often used to forward inputs to the next step in a chain."
      ],
      "metadata": {
        "id": "0qosz5XtrXmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "message = \"\"\"\n",
        "Answer this question using the provided context only.\n",
        "\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "L4GFdtQvp12x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])"
      ],
      "metadata": {
        "id": "6GLeFU_Bqgub"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "message: This is a template for the prompt that will be sent to the language model. It includes placeholders for:\n",
        "\n",
        "{question}: The user's question.\n",
        "\n",
        "{context}: The relevant context retrieved from a knowledge source (e.g., a vector store).\n",
        "\n",
        "ChatPromptTemplate.from_messages([(\"human\", message)]): Creates a ChatPromptTemplate object from the message template. The (\"human\", message) indicates that the message is from a human user.\n",
        "\n",
        "\n",
        "{\"context\": retriever, \"question\": RunnablePassthrough()}:\n",
        "\n",
        "retriever: This is the retriever object (from your previous code) that fetches relevant documents from a vector store based on the user's question.\n",
        "\n",
        "RunnablePassthrough(): This forwards the user's question ({question}) to the next step in the pipeline without modifying it.\n",
        "\n",
        "prompt: The ChatPromptTemplate created earlier. It takes the context (retrieved documents) and question (user's input) and formats them into a structured prompt for the language model.\n",
        "\n",
        "llm: This is the language model (e.g., OpenAI's GPT, Anthropic's Claude, etc.) that generates a response based on the formatted prompt.\n",
        "\n",
        "The | operator is used to chain these components together, creating a pipeline where data flows from one step to the next."
      ],
      "metadata": {
        "id": "xX1xq94Hsbvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n"
      ],
      "metadata": {
        "id": "0zd7kt-hqiNU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How It Works Together\n",
        "The user asks a question: \"tell about shark?\".\n",
        "\n",
        "The retriever fetches the most relevant document(s) about sharks from the vector store.\n",
        "\n",
        "The prompt template combines the question and retrieved context into a structured prompt.\n",
        "\n",
        "The language model generates a response based on the prompt.\n",
        "\n",
        "The response is printed to the user."
      ],
      "metadata": {
        "id": "Eeuwu-dfr-g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"tell about gold?\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQX3MlZ5qoPd",
        "outputId": "1148f9ed-cb43-4677-f9e1-92e3c7d04e10"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text only mentions goldfish, a type of fish that is a popular pet.  There is no information about gold as a metal or other substance.\n"
          ]
        }
      ]
    }
  ]
}